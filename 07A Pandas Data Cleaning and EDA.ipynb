{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red\"> THIS COPY IS FILLED OUT</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center; font-size: 5em;\">Exploratory Data Analysis<br>Pandas & Numpy</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"anchorWhatIsNumPy\" style=\"position:absolute;\"></a>\n",
    "<hr style=\"border:2px solid\">\n",
    "<h1 style=\"text-align:center; font-size: 3em;\">Workbook Overview</h1>\n",
    "<hr style=\"border-top:1px dashed\">\n",
    "\n",
    "In this lesson, we'll explore Pandas for EDA. Specifically: \n",
    "\n",
    "- Identify and handle missing values with Pandas.\n",
    "- Implement groupby statements for specific segmented analysis.\n",
    "- Use apply functions to clean data with Pandas.\n",
    "\n",
    "We'll implicitly review many functions from our first Pandas lesson along the way!\n",
    "\n",
    "<ul>\n",
    "        <li><a href=\"#anchorDataset\">AdventureWorks Cycles Dataset</a></li> \n",
    "        <li><a href=\"#anchorMissingData\">Finding Missing Data</a></li> \n",
    "        <li><a href=\"#anchorGroupby\">GroupBy</a></li> \n",
    "        <li><a href=\"#anchorApply\">Apply Function</a></li> \n",
    "        <li><a href=\"#anchorYourTurn\">Practice</a></li> \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top:1px dashed\">\n",
    "\n",
    "## Import Libraries\n",
    "We will be mainly using Pandas as well as functions from Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:37:23.683332Z",
     "start_time": "2024-01-25T12:37:22.631857Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"anchorDataset\" style=\"position:absolute;\"></a>\n",
    "<hr style=\"border:2px solid\">\n",
    "\n",
    "## AdventureWorks Cycles Dataset\n",
    "<hr style=\"border-top:1px dashed\">\n",
    "\n",
    "### Do you know the AdventureWorks Cycles Dataset?\n",
    "<img align=\"right\" src=\"http://lh6.ggpht.com/_XjcDyZkJqHg/TPaaRcaysbI/AAAAAAAAAFo/b1U3q-qbTjY/AdventureWorks%20Logo%5B5%5D.png?imgmax=800\">\n",
    "\n",
    "Here's the Production.Product table [data dictionary](https://www.sqldatadictionary.com/AdventureWorks2014/Production.Product.html), which is a description of the fields (columns) in the table (the .csv file we will import below):<br>\n",
    "- **ProductID** - Primary key for Product records.\n",
    "- **Name** - Name of the product.\n",
    "- **ProductNumber** - Unique product identification number.\n",
    "- **MakeFlag** - 0 = Product is purchased, 1 = Product is manufactured in-house.\n",
    "- **FinishedGoodsFlag** - 0 = Product is not a salable item. 1 = Product is salable.\n",
    "- **Color** - Product color.\n",
    "- **SafetyStockLevel** - Minimum inventory quantity.\n",
    "- **ReorderPoint** - Inventory level that triggers a purchase order or work order.\n",
    "- **StandardCost** - Standard cost of the product.\n",
    "- **ListPrice** - Selling price.\n",
    "- **Size** - Product size.\n",
    "- **SizeUnitMeasureCode** - Unit of measure for the Size column.\n",
    "- **WeightUnitMeasureCode** - Unit of measure for the Weight column.\n",
    "- **DaysToManufacture** - Number of days required to manufacture the product.\n",
    "- **ProductLine** - R = Road, M = Mountain, T = Touring, S = Standard\n",
    "- **Class** - H = High, M = Medium, L = Low\n",
    "- **Style** - W = Womens, M = Mens, U = Universal\n",
    "- **ProductSubcategoryID** - Product is a member of this product subcategory. Foreign key to ProductSubCategory.ProductSubCategoryID.\n",
    "- **ProductModelID** - Product is a member of this product model. Foreign key to ProductModel.ProductModelID.\n",
    "- **SellStartDate** - Date the product was available for sale.\n",
    "- **SellEndDate** - Date the product was no longer available for sale.\n",
    "- **DiscontinuedDate** - Date the product was discontinued.\n",
    "- **rowguid** - ROWGUIDCOL number uniquely identifying the record. Used to support a merge replication sample.\n",
    "- **ModifiedDate** - Date and time the record was last updated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"anchorInitialImpressions\" style=\"position:absolute;\"></a>\n",
    "<hr style=\"border:2px solid\">\n",
    "\n",
    "## Initial Impressions\n",
    "<hr style=\"border-top:1px dashed\">\n",
    "\n",
    "For our data, we are going to use the `read_csv()` function. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Question:</b> What does CSV stand for?\n",
    "</div>\n",
    "\n",
    "If you look at our data file, you can see it is separated by tabs instead. When you type out `.read_csv()` press `Shift+Tab` while having your cursor in the parenthesis. One of the parameters we can use to help us is called `sep=`\n",
    "\n",
    "That being said, we are going to use the `'\\t'` separator to specify tab-delimited columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:37:24.137059Z",
     "start_time": "2024-01-25T12:37:24.085912Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we have a new dataset, it's always a good idea to look at the top couple of rows once loaded in.\n",
    "\n",
    "Let's check out the first 3 rows using `.head()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:37:25.229675Z",
     "start_time": "2024-01-25T12:37:25.181537Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we instead wanted to take a look at the last couple of rows, we would use `tail()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:37:26.181712Z",
     "start_time": "2024-01-25T12:37:26.154792Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have several other functions that let us take a look at the dataframe's aspects\n",
    "- `.shape`\n",
    "- `.dtypes`\n",
    "- `.info()`\n",
    "- `.describe()`\n",
    "\n",
    "Let's explore them real quick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:37:27.212928Z",
     "start_time": "2024-01-25T12:37:27.197992Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:37:28.023378Z",
     "start_time": "2024-01-25T12:37:28.001378Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:37:28.351203Z",
     "start_time": "2024-01-25T12:37:28.328838Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:37:28.724529Z",
     "start_time": "2024-01-25T12:37:28.655445Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Columns can be misread as \"objects\" when they are really floats. If that is the case, use <code>astype()</code> to cast a column as a different type.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Using these functions together at the start of EDA can save hours.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice anything special about `ProductID`? Use the function `nunique()` to count the distinct values over our `ProductID` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:37:31.409111Z",
     "start_time": "2024-01-25T12:37:31.388897Z"
    }
   },
   "outputs": [],
   "source": [
    "prod['ProductID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our shape says there are 504 columns, and there are 504 unique values in this column (as well as the data dictionary telling us that it is the primary key) it is safe to say that this is indeed the primary key.\n",
    "\n",
    "That being the case, let's bring our `ProductID` column into the index since it's the PK (primary key) of our table and that's where PKs belong as a best practice.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Question:</b> How would we move the <code>ProductID</code> column into the index column?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:37:33.219758Z",
     "start_time": "2024-01-25T12:37:33.182362Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:37:34.259121Z",
     "start_time": "2024-01-25T12:37:34.250593Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `.head()`, let's make sure that worked and there aren't any extra parameters `**cough cough**` that we might need to change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:37:38.354514Z",
     "start_time": "2024-01-25T12:37:38.316225Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can access each row of data using .loc[ProductID] for example: `prod.loc[3]` retrieves product 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:39:15.935523Z",
     "start_time": "2024-01-25T12:39:15.921475Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"anchorMissingData\" style=\"position:absolute;\"></a>\n",
    "<hr style=\"border:2px solid\">\n",
    "\n",
    "## Handling Missing Data\n",
    "<hr style=\"border-top:1px dashed\">\n",
    "\n",
    "\n",
    "Recall missing data is a systemic, challenging problem for data scientists. Imagine conducting a poll, but some of the data gets lost, or you run out of budget and can't complete it! 😮<br><br>\n",
    "\n",
    "\"Handling missing data\" itself is a broad topic. We'll focus on two components:\n",
    "\n",
    "- Using Pandas to identify we have missing data\n",
    "- Strategies to fill in missing data (known in the business as `imputing`)\n",
    "- Filling in missing data with Pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Identifying missing data\n",
    "\n",
    "Before *handling*, we must identify we're missing data at all!\n",
    "\n",
    "We have a few ways to explore missing data, and they are reminiscient of our Boolean filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:40:15.735631Z",
     "start_time": "2024-01-25T12:40:15.705978Z"
    }
   },
   "outputs": [],
   "source": [
    "prod.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:39:45.818209Z",
     "start_time": "2024-01-25T12:39:45.780296Z"
    }
   },
   "outputs": [],
   "source": [
    "# True when data isn't missing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:41:00.554129Z",
     "start_time": "2024-01-25T12:41:00.517006Z"
    }
   },
   "outputs": [],
   "source": [
    "# True when data is missing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we may want to see null values in aggregate. We can use `sum()` to sum down a given column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:41:09.468260Z",
     "start_time": "2024-01-25T12:41:09.456589Z"
    }
   },
   "outputs": [],
   "source": [
    "# here is a quick way to do it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:41:16.134784Z",
     "start_time": "2024-01-25T12:41:16.110747Z"
    }
   },
   "outputs": [],
   "source": [
    "prod.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Look! We've found missing values!</b>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Question:</b> How could this missing data be problematic for our analysis?\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>DataSet Null Characters:</b> Did you find no nulls in your data? <br>Some datasets will come with other characters instead of <code>Nulls</code> you can use the <code>.replace()</code> to replace those characters with <code>np.nan</code> instead.<br> It is almost impossible to find perfect Data, better to check again then miss null characters like a <code>?</code> acting as an invisible null.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Understanding missing data\n",
    "\n",
    "Finding missing data is the easy part! Determining way to do next is more complicated.\n",
    "\n",
    "Typically, we are most interested in knowing **why** we are missing data. Once we know what 'type of missingness' we have (the source of missing data), we can proceed effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first quantify how much data we are missing. Here is another implementation of `prod.isnull().sum()`, only wrapped with a `DataFrame` and some labels to make it a little more user-friendly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:41:57.995298Z",
     "start_time": "2024-01-25T12:41:57.977444Z"
    }
   },
   "outputs": [],
   "source": [
    "null_df = pd.DataFrame(prod.isnull().sum(), columns=['Count of Nulls']) #Creating a new DF\n",
    "\n",
    "null_df.index.name = 'Column Name' # Changing the Index Name\n",
    "\n",
    "null_df.sort_values(['Count of Nulls'], ascending=False) # Sorting based on our only column name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Filling in missing data\n",
    "\n",
    "How we fill in data depends largely on why it is missing (types of missingness) and what sampling we have available to us.\n",
    "\n",
    "We may:\n",
    "\n",
    "- Delete missing data altogether\n",
    "- Fill in missing data with:\n",
    "    - The average of the column\n",
    "    - The median of the column\n",
    "    - A predicted amount based on other factors\n",
    "- Collect more data:\n",
    "    - Resample the population\n",
    "    - Followup with the authority providing data that is missing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, let's focus on handling missing values in `Color`. Let's get a count of the unique values in that column. We will need to use the `dropna=False` kwarg, otherwise the `pd.Series.value_counts()` method will not count `NaN` (null) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:42:31.309527Z",
     "start_time": "2024-01-25T12:42:31.286790Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Option 1: Drop the missing values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:42:45.591998Z",
     "start_time": "2024-01-25T12:42:45.575483Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:42:48.890436Z",
     "start_time": "2024-01-25T12:42:48.868213Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important!** `pd.DataFrame.dropna()` and `pd.Series.dropna()` are very versatile! Let's look at the docs (Series is similar):\n",
    "\n",
    "```python\n",
    "Signature: pd.DataFrame.dropna(self, axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "Docstring:\n",
    "Remove missing values.\n",
    "\n",
    "See the :ref:`User Guide <missing_data>` for more on which values are\n",
    "considered missing, and how to work with missing data.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "axis : {0 or 'index', 1 or 'columns'}, default 0\n",
    "    Determine if rows or columns which contain missing values are\n",
    "    removed.\n",
    "\n",
    "    * 0, or 'index' : Drop rows which contain missing values.\n",
    "    * 1, or 'columns' : Drop columns which contain missing value.\n",
    "\n",
    "    .. deprecated:: 0.23.0: Pass tuple or list to drop on multiple\n",
    "    axes.\n",
    "how : {'any', 'all'}, default 'any'\n",
    "    Determine if row or column is removed from DataFrame, when we have\n",
    "    at least one NA or all NA.\n",
    "\n",
    "    * 'any' : If any NA values are present, drop that row or column.\n",
    "    * 'all' : If all values are NA, drop that row or column.\n",
    "thresh : int, optional\n",
    "    Require that many non-NA values.\n",
    "subset : array-like, optional\n",
    "    Labels along other axis to consider, e.g. if you are dropping rows\n",
    "    these would be a list of columns to include.\n",
    "inplace : bool, default False\n",
    "    If True, do operation inplace and return None.\n",
    "```\n",
    "\n",
    "**how**: This tells us if we want to remove a row if _any_ of the columns have a null, or _all_ of the columns have a null.<br>\n",
    "**subset**: We can input an array here, like `['Color', 'Size', 'Weight']`, and it will only consider nulls in those columns. This is very useful!<br>\n",
    "**inplace**: This is if you want to mutate (change) the source dataframe. Default is `False`, so it will return a _copy_ of the source dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To accomplish the same thing, but implement it on our entire dataframe, we can do the following:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:43:09.877307Z",
     "start_time": "2024-01-25T12:43:09.828426Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Option 2: Fill in missing values**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditionally, we fill missing data with a median, average, or mode (most frequently occurring). For `Color`, let's replace the nulls with the string value `NoColor`.\n",
    "\n",
    "Let's first look at the way we'd do it with a single column, using the `pd.Series.fillna()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:43:28.503291Z",
     "start_time": "2024-01-25T12:43:28.482674Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how we'd do it to the whole dataframe, using the `pd.DataFrame.fillna()` method. Notice the similar application program interface between the two methods with the `value` named argument being passed. \n",
    "\n",
    "<br>Nice job to the pandas development team! \n",
    "\n",
    "<br>The full dataframe is returned, and not just a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:43:37.496798Z",
     "start_time": "2024-01-25T12:43:37.444958Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can reference any other data or formulas we want with the value we fill the nulls with. This is very handy if you want to impute with the average or median of that column... or even another column altogether! Here is an example where we will the nulls of `Color` with the average value from the `ListPrice` column. *This has no practical value in this application, but immense value in other applications.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:43:59.825881Z",
     "start_time": "2024-01-25T12:43:59.787891Z"
    }
   },
   "outputs": [],
   "source": [
    "prod.fillna(value={'Color': prod['ListPrice'].mean()}).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:44:03.753506Z",
     "start_time": "2024-01-25T12:44:03.717992Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Are the nulls gone in <code>Color</code>?</b> If not:\n",
    "    <ul><li>Don't forget to use the <code>inplace=True</code> kwarg to mutate the source dataframe (i.e. 'save changes'). \n",
    "        <li> It is helpful to not use <code>inplace=True</code> initially to ensure your code/logic is correct, prior to making permanent changes.\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:45:49.225999Z",
     "start_time": "2024-01-25T12:45:49.207390Z"
    }
   },
   "outputs": [],
   "source": [
    "prod.fillna(value={'Color':'NoColor'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"anchorGroupby\" style=\"position:absolute;\"></a>\n",
    "<hr style=\"border:2px solid\">\n",
    "\n",
    "## Groupby Statements\n",
    "<hr style=\"border-top:1px dashed\">\n",
    "\n",
    "In Pandas, groupby statements are similar to pivot tables in that they allow us to segment our population to a specific subset.\n",
    "\n",
    "For example, if we want to know the average number of bottles sold and pack sizes per city, a groupby statement would make this task much more straightforward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To think how a groupby statement works, think about it like this:\n",
    "\n",
    "- **Split:** Separate our DataFrame by a specific attribute, for example, group by `Color`\n",
    "- **Combine:** Put our DataFrame back together and return some _aggregated_ metric, such as the `sum`, `count`, or `max`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://i.imgur.com/yjNkiwL.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's group by `Color`, and get a count of products for each color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T13:01:11.977922Z",
     "start_time": "2024-01-25T13:01:11.968184Z"
    }
   },
   "outputs": [],
   "source": [
    "# group by Color, giving the number of products of each color\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we notice about this output? Are all columns the same? Why or why not?\n",
    "\n",
    "We can see that the `.count()` method excludes nulls, and there is no way to change this with the current implementation:\n",
    "```python\n",
    "Signature: .count()\n",
    "Docstring: Compute count of group, excluding missing values \n",
    "File:      ~/miniconda3/envs/ga/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\n",
    "Type:      method\n",
    "```\n",
    "\n",
    "As a best practice, you should fill in nulls prior to your .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:46:21.191632Z",
     "start_time": "2024-01-25T12:46:21.137667Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find out the most expensive price for an item, by `Color`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:47:18.094743Z",
     "start_time": "2024-01-25T12:47:18.078835Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:46:58.186085Z",
     "start_time": "2024-01-25T12:46:58.148173Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prod[['Color', 'ListPrice']].groupby('Color').max().sort_values('ListPrice', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do multi-level groupbys. This is referred to as a `Multiindex` dataframe. Here, we can see the following fields in a nested group by, with a count of Name (with nulls filled!); effectively giving us a count of the number of products for every unique Class/Style combination:\n",
    "\n",
    "- Class - H = High, M = Medium, L = Low\n",
    "- Style - W = Womens, M = Mens, U = Universal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:48:33.352900Z",
     "start_time": "2024-01-25T12:48:33.314886Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T13:02:49.017727Z",
     "start_time": "2024-01-25T13:02:49.002334Z"
    }
   },
   "outputs": [],
   "source": [
    "prod.fillna(value={'Name':'X'}).groupby(by=['Style']).count()[['Name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `.agg()` method with multiple arguments, to simulate a `.describe()` method like we used before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:50:18.986499Z",
     "start_time": "2024-01-25T12:50:18.970167Z"
    }
   },
   "outputs": [],
   "source": [
    "listAgg = ['count']\n",
    "\n",
    "listAgg.append('mean')\n",
    "print(listAgg)\n",
    "\n",
    "prod.groupby('Color')['ListPrice'].agg(listAgg).sort_values(listAgg[-1], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"anchorApply\" style=\"position:absolute;\"></a>\n",
    "<hr style=\"border:2px solid\">\n",
    "\n",
    "## Apply Function\n",
    "<hr style=\"border-top:1px dashed\">\n",
    "\n",
    "Apply functions allow us to perform a complex operation across an entire columns highly efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's say we want to change our colors from a word, to just a single letter. How would we do that?\n",
    "\n",
    "The first step is writing a function, with the argument being the value we would receive from each cell in the column. This function will mutate the input, and return the result. This result will then be _applied_ to the source dataframe (if desired)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:50:28.149893Z",
     "start_time": "2024-01-25T12:50:28.128408Z"
    }
   },
   "outputs": [],
   "source": [
    "prod['Color'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:50:30.631512Z",
     "start_time": "2024-01-25T12:50:30.610500Z"
    }
   },
   "outputs": [],
   "source": [
    "prod['Color'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:50:43.757450Z",
     "start_time": "2024-01-25T12:50:43.746524Z"
    }
   },
   "outputs": [],
   "source": [
    "def color_to_letter(color):\n",
    "    color_dict = {\n",
    "        'Black': 'B', \n",
    "        'Silver': 'S', \n",
    "        'Red': 'R', \n",
    "        'White': 'W', \n",
    "        'Blue': 'Bl', \n",
    "        'Multi': 'M', \n",
    "        'Yellow': 'Y',\n",
    "        'Grey': 'G', \n",
    "        'Silver/Black': 'SB'\n",
    "    }\n",
    "    \n",
    "    list_of_keys = color_dict.keys()\n",
    "    \n",
    "    if color in list_of_keys:\n",
    "        return color_dict[color]\n",
    "    else:\n",
    "        return 'N'\n",
    "    \n",
    "    # Alternative Version of if Statment\n",
    "#     try:\n",
    "#         return color_dict[color] # Try to run >> has error >> Except\n",
    "#     except:\n",
    "#         return 'N' #An Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_to_letter(\"Blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_to_letter(\"Orange\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can _apply_ this function to our `pd.Series` object, returning the result (which we can use to overwrite the source, if we choose)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:51:19.273760Z",
     "start_time": "2024-01-25T12:51:19.250886Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pd.DataFrame.apply` implementation is similar, however it effectively 'scrolls through' the columns and passes each one sequentially to your function:\n",
    "\n",
    "```python\n",
    "Objects passed to the function are Series objects whose index is\n",
    "either the DataFrame's index (``axis=0``) or the DataFrame's columns\n",
    "(``axis=1``).\n",
    "```\n",
    "\n",
    "It should only be used when you wish to apply the same function to all columns (or rows) of your `pd.DataFrame` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `pd.Series.apply()` with a **labmda expression**. This is an undeclared function and is commonly used for simple functions within the `.apply()` method. Let's use it to add $100 to our `ListPrice` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:51:30.259158Z",
     "start_time": "2024-01-25T12:51:30.243239Z"
    }
   },
   "outputs": [],
   "source": [
    "# without apply\n",
    "prod['ListPrice'].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:51:42.486362Z",
     "start_time": "2024-01-25T12:51:42.465039Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# and now with 100 more dollars!\n",
    "prod['ListPrice'].apply(lambda x: x+100).tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T12:51:47.317472Z",
     "start_time": "2024-01-25T12:51:47.267751Z"
    }
   },
   "outputs": [],
   "source": [
    "prod.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid\">\n",
    "\n",
    "## Conclusion\n",
    "<hr style=\"border-top:1px dashed\">\n",
    "We've covered even more useful information! Here are the key takeaways:\n",
    "\n",
    "- **Missing data** comes in many shapes and sizes. Before deciding how to handle it, we identify it exists. We then derive how the missingness is affecting our dataset, and make a determination about how to fill in values.\n",
    "\n",
    "```python\n",
    "# pro tip for identifying missing data\n",
    "df.isnull().sum()\n",
    "```\n",
    "\n",
    "- **Grouby** statements are particularly useful for a subsection-of-interest analysis. Specifically, zooming in on one condition, and determining relevant statstics.\n",
    "\n",
    "```python\n",
    "# group by \n",
    "df.groupby('column').agg['count', 'mean', 'max', 'min']\n",
    "```\n",
    "\n",
    "- **Apply functions** help us clean values across an entire DataFrame column. They are *like* a for loop for cleaning, but many times more efficient. They follow a common pattern:\n",
    "    1. Write a function that works on a single value\n",
    "    2. Test that function on a single value\n",
    "    3. Apply that function to a whole column\n",
    "\n",
    "(The most confusing part of apply functions is that we write them with *a single value* in mind, and then apply them to many single values at once.)\n",
    "<hr style=\"border:2px solid\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
